{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open python and nltk packages needed for processing\n",
    "# while the semeval tweet task b data has tags for \"positive\", \"negative\", \n",
    "#  \"objective\", \"neutral\", \"objective-OR-neutral\", we will combine the last 3 into \"neutral\"\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "\n",
    "\n",
    "# function to read tweet training file, train and test a classifier \n",
    "def processtweets():\n",
    "  # convert the limit argument from a string to an int\n",
    "  # initialize NLTK built-in tweet tokenizer\n",
    "  twtokenizer = TweetTokenizer()\n",
    "  \n",
    "  f = open('C:/Users/jiebi/Desktop/Grad/Summer 2021/IST 664/Final/corpus/downloaded-tweeti-b-dist.tsv', 'r')\n",
    "  # loop over lines in the file and use the first limit of them\n",
    "  #    assuming that the tweets are sufficiently randomized\n",
    "  tweetdata = []\n",
    "  for line in f:\n",
    "      line = str(line).replace('@', '')\n",
    "      line = str(line).replace('#', '')\n",
    "      line = line.strip()\n",
    "\n",
    "      # each line has 4 items separated by tabs\n",
    "      # ignore the tweet and user ids, and keep the sentiment and tweet text\n",
    "      tweetdata.append(line.split('\\t')[2:4])\n",
    "  # create list of tweet documents as (list of words, label)\n",
    "  # where the labels are condensed to just 3:  'pos', 'neg', 'neu'\n",
    "  tweetdocs = []\n",
    "  # add all the tweets except the ones whose text is Not Available\n",
    "  for tweet in tweetdata:\n",
    "    if (tweet[1] != 'Not Available'):\n",
    "      # run the tweet tokenizer on the text string - returns unicode tokens, so convert to utf8\n",
    "      tokens = twtokenizer.tokenize(tweet[1])\n",
    "\n",
    "      if tweet[0] == '\"positive\"':\n",
    "        label = 'pos'\n",
    "      else:\n",
    "        if tweet[0] == '\"negative\"':\n",
    "          label = 'neg'\n",
    "        else:\n",
    "          if (tweet[0] == '\"neutral\"') or (tweet[0] == '\"objective\"') or (tweet[0] == '\"objective-OR-neutral\"'):\n",
    "            label = 'neu'\n",
    "          else:\n",
    "            label = ''\n",
    "      tweetdocs.append((tokens, label))\n",
    "\n",
    "  return tweetdocs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets_doc = processtweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_measures(gold, predicted, labels):\n",
    "    \n",
    "    # these lists have values for each label \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        # for small numbers, guard against dividing by zero in computing measures\n",
    "        if (TP == 0) or (FP == 0) or (FN == 0):\n",
    "          recall_list.append (0)\n",
    "          precision_list.append (0)\n",
    "          F1_list.append(0)\n",
    "        else:\n",
    "          recall = TP / (TP + FP)\n",
    "          precision = TP / (TP + FN)\n",
    "          recall_list.append(recall)\n",
    "          precision_list.append(precision)\n",
    "          F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    return (precision_list, recall_list, F1_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_PRF(num_folds, featuresets, labels):\n",
    "    subset_size = int(len(featuresets)/num_folds)\n",
    "    print('Each fold size:', subset_size)\n",
    "    # for the number of labels - start the totals lists with zeroes\n",
    "    num_labels = len(labels)\n",
    "    total_precision_list = [0] * num_labels\n",
    "    total_recall_list = [0] * num_labels\n",
    "    total_F1_list = [0] * num_labels\n",
    "\n",
    "    # iterate over the folds\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[(i*subset_size):][:subset_size]\n",
    "        train_this_round = featuresets[:(i*subset_size)] + featuresets[((i+1)*subset_size):]\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "        goldlist = []\n",
    "        predictedlist = []\n",
    "        for (features, label) in test_this_round:\n",
    "            goldlist.append(label)\n",
    "            predictedlist.append(classifier.classify(features))\n",
    "        \n",
    "        (precision_list, recall_list, F1_list) \\\n",
    "                  = eval_measures(goldlist, predictedlist, labels)\n",
    "\n",
    "        # for each label add to the sums in the total lists\n",
    "        for i in range(num_labels):\n",
    "            # for each label, add the 3 measures to the 3 lists of totals\n",
    "            total_precision_list[i] += precision_list[i]\n",
    "            total_recall_list[i] += recall_list[i]\n",
    "            total_F1_list[i] += F1_list[i]\n",
    "\n",
    "    precision_list = [tot/num_folds for tot in total_precision_list]\n",
    "    recall_list = [tot/num_folds for tot in total_recall_list]\n",
    "    F1_list = [tot/num_folds for tot in total_F1_list]\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('\\nAverage Precision\\tRecall\\t\\tF1 \\tPer Label')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))\n",
    "    \n",
    "    # print macro average over all labels - treats each label equally\n",
    "    print('\\nMacro Average Precision\\tRecall\\t\\tF1 \\tOver All Labels')\n",
    "\n",
    "    p_dummy = 0\n",
    "    r_dummy = 0\n",
    "    f_dummy = 0\n",
    "    \n",
    "    for x in precision_list:\n",
    "        p_dummy = p_dummy + x\n",
    "    \n",
    "    for x in recall_list:\n",
    "        r_dummy = r_dummy + x\n",
    "    \n",
    "    for x in F1_list:\n",
    "        f_dummy = f_dummy + x\n",
    "    \n",
    "    print('\\t', \"{:10.3f}\".format(p_dummy/num_labels), \\\n",
    "          \"{:10.3f}\".format(r_dummy/num_labels), \\\n",
    "          \"{:10.3f}\".format(f_dummy/num_labels))\n",
    "\n",
    "    # for micro averaging, weight the scores for each label by the number of items\n",
    "    #    this is better for labels with imbalance\n",
    "    # first intialize a dictionary for label counts and then count them\n",
    "    label_counts = {}\n",
    "    for lab in labels:\n",
    "      label_counts[lab] = 0 \n",
    "    # count the labels\n",
    "    for (doc, lab) in featuresets:\n",
    "      label_counts[lab] += 1\n",
    "    # make weights compared to the number of documents in featuresets\n",
    "    num_docs = len(featuresets)\n",
    "    label_weights = [(label_counts[lab] / num_docs) for lab in labels]\n",
    "    print('\\nLabel Counts', label_counts)\n",
    "    #print('Label weights', label_weights)\n",
    "    # print macro average over all labels\n",
    "    print('Micro Average Precision\\tRecall\\t\\tF1 \\tOver All Labels')\n",
    "    \n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    F1 = 0\n",
    "    for a,b in zip(precision_list, label_weights):\n",
    "        precision = precision + a*b\n",
    "        \n",
    "    for a,b in zip(recall_list, label_weights):\n",
    "        recall = recall + a*b\n",
    "        \n",
    "    for a,b in zip(F1_list, label_weights):\n",
    "        F1 = F1 + a*b\n",
    "    \n",
    "\n",
    "    print( '\\t', \"{:10.3f}\".format(precision), \\\n",
    "      \"{:10.3f}\".format(recall), \"{:10.3f}\".format(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_measures(gold, predicted, labels):\n",
    "    \n",
    "    # these lists have values for each label \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        # for small numbers, guard against dividing by zero in computing measures\n",
    "        if (TP == 0) or (FP == 0) or (FN == 0):\n",
    "          recall_list.append (0)\n",
    "          precision_list.append (0)\n",
    "          F1_list.append(0)\n",
    "        else:\n",
    "          recall = TP / (TP + FP)\n",
    "          precision = TP / (TP + FN)\n",
    "          recall_list.append(recall)\n",
    "          precision_list.append(precision)\n",
    "          F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    return (precision_list, recall_list, F1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [word for (sentance,category) in all_tweets_doc for word in sentance]\n",
    "top_words = nltk.FreqDist(all_words)\n",
    "most_common_words = top_words.most_common(2000)\n",
    "word_features = [word for (word,count) in most_common_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets_uni = [(document_features(d, word_features), c) for (d, c) in all_tweets_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the POS features function\n",
    "def POS_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    tagged_words = nltk.pos_tag(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    numNoun = 0\n",
    "    numVerb = 0\n",
    "    numAdj = 0\n",
    "    numAdverb = 0\n",
    "    for (word, tag) in tagged_words:\n",
    "        if tag.startswith('N'): numNoun += 1\n",
    "        if tag.startswith('V'): numVerb += 1\n",
    "        if tag.startswith('J'): numAdj += 1\n",
    "        if tag.startswith('R'): numAdverb += 1\n",
    "    features['nouns'] = numNoun\n",
    "    features['verbs'] = numVerb\n",
    "    features['adjectives'] = numAdj\n",
    "    features['adverbs'] = numAdverb\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSfeaturesets = [(POS_features(d, word_features), c) for (d, c) in all_tweets_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [c for (d,c) in all_tweets_doc]\n",
    "labels = list(set(label_list))    # gets only unique labels\n",
    "num_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original featureset\n",
      "Each fold size: 1641\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "pos \t      0.636      0.680      0.654\n",
      "neg \t      0.523      0.396      0.447\n",
      "neu \t      0.675      0.706      0.686\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.611      0.594      0.596\n",
      "\n",
      "Label Counts {'pos': 3059, 'neg': 1207, 'neu': 3942}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.638      0.651      0.639\n"
     ]
    }
   ],
   "source": [
    "print(\"Original featureset\")\n",
    "cross_validation_PRF(num_folds, featuresets_uni, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS featureset\n",
      "Each fold size: 1641\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "pos \t      0.615      0.679      0.643\n",
      "neg \t      0.541      0.379      0.442\n",
      "neu \t      0.669      0.707      0.683\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.608      0.589      0.589\n",
      "\n",
      "Label Counts {'pos': 3059, 'neg': 1207, 'neu': 3942}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.630      0.649      0.633\n"
     ]
    }
   ],
   "source": [
    "print(\"POS featureset\")\n",
    "cross_validation_PRF(num_folds, POSfeaturesets, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tweets_doc = []\n",
    "for x,y in all_tweets_doc:\n",
    "    new_tweets_doc.append([x,y])\n",
    "\n",
    "for i,x in new_tweets_doc:\n",
    "    for tokens in range(len(i)):\n",
    "        i[tokens] = i[tokens].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend([line.strip() for line in open('C:/Users/jiebi/Desktop/Grad/Summer 2021/IST 664/Final/stopwords_twitter.txt')])\n",
    "\n",
    "newstopwords = [word for word in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_all_words_list = [word for (sent,cat) in new_tweets_doc for word in sent if word not in newstopwords]\n",
    "new_all_words = nltk.FreqDist(new_all_words_list)\n",
    "new_word_items = new_all_words.most_common(2000)\n",
    "new_word_features = [word for (word,count) in new_word_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_POSfeaturesets = [(document_features(d, new_word_features), c) for (d, c) in new_tweets_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW POS featureset\n",
      "Each fold size: 1641\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "pos \t      0.644      0.668      0.652\n",
      "neg \t      0.448      0.350      0.390\n",
      "neu \t      0.654      0.689      0.666\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.582      0.569      0.569\n",
      "\n",
      "Label Counts {'pos': 3059, 'neg': 1207, 'neu': 3942}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.620      0.631      0.620\n"
     ]
    }
   ],
   "source": [
    "print(\"NEW POS featureset\")\n",
    "cross_validation_PRF(num_folds, new_POSfeaturesets, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
